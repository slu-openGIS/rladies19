---
title: "RLadies 2019"
author: "Christopher Prener, Ph.D."
date: '(`r format(Sys.time(), "%B %d, %Y")`)'
output: 
  github_document: default
  html_notebook: default 
---

## Introduction
This notebook provides an overview of some of the features and tools that are at the core of the `tidystl` framework. More about `tidystl` can be found on the [SLU openGIS website](https://slu-opengis.github.io/news/tidystl_annoucement/).

## Dependencies
This notebook requires a variety of `tidystl`, `tidyverse`, and other dependencies. These reflect the ways in which I manage my own research and teach students to use `R`. There are other frameworks both for data wrangling (i.e. base `R`, `dt` (datatable)) and spatial data management (i.e. `sp`). The code chunks in the [README.md file](https://github.com/slu-openGIS/rladies19/blob/master/README.md) include installation code for all dependencies.

```{r load-packages}
# tidystl packages
library(compstatr)     # stlmpd data wrangling
library(gateway)       # st louis spatial data tools
library(postmastr)     # street address normalization
library(stlcsb)        # st louis citizens service bureau data wrangling

# tidyverse packages
library(dplyr)         # data wrangling
library(ggplot2)       # plotting
library(readr)
library(stringr)

# spatial packages
library(sf)            # spatial data tools

# other packages
library(here)          # file path management
library(testthat)      # unit testing
```

There is also a function that is not yet part of the `compstatr` package that we'll use today:

```{r load-combine-address}
source(here("source", "combine_address.R"))
```


## Load Data
This notebook requires a number of data sets, which we'll load in groups based on their purpose.

### Geocoders
Two local geocoders are saved as `.rda` files in the `data/` to provide the most efficient storage means possible:

```{r load-geocoders}
# load local full geocoder
load(here("data", "geocoder.rda"))

# load local short geocoder
load(here("data", "geocoder_s.rda"))
```

Both geocoders could also be built using the `gateway` package:

```r
# build local full geocoder
geocoder <- gw_build_geocoder(style = "full", class = "tibble", return = "coords")

# build local short goecoder
geocoder_s <- gw_build_geocoder(style = "short", class = "tibble", return = "coords")
```

These are not built here to save time during the tutorial.

### Citizens' Service Bureau Data
A data set of non-emergency service requests (things like concerns about potholes, traffic lights that are out, and vacant buildings) for the years 2017 and 2018 is included in this repository:

```{r load-csb}
csb <- read_csv(here("data", "csb.csv"))
```

These data can also be downloaded directly via the `stlcsb` package's `csb_get_data()` function:

```r
csb <- csb_get_data(years = 2017:2018)
```

The `years` argument is optional.

With the data loaded, we'll subset the number of columns to reduce the amount of data we need to wade through and make an edit to the address field:

```{r}
csb %>% 
  select(datetimeinit, probaddress, problemcode, status) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = " - ", replacement = "-")) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = "- ", replacement = "-")) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = " -", replacement = "-")) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = "MO-", replacement = "MO ")) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = "I-", replacement = "I ")) -> csb
```


### Crime Data
The crime data we'll be looking at today, also for 2017 and 2018, is included in its raw form in `data/crime/`. These data have been downloaded from the St. Louis Metropolitan Police Department's [crime statistics website](https://www.slmpd.org/Crimereports.shtml). Because of the structure of the site, we don't have a function at this time the reads the files into `R` direction from the web. They must be downloaded manually and then can be read in using the workflow below.

The `compstatr` package is "opinionated" and requires a particular organizational approach - monthly crime files should be stored as a package in dedicated yearly folders. Notice that the files have the `.html` file extension mistakenly appended:

```{r list-2017}
list.files(path = here("data", "crime", "2017"))
```

We can fix this with `cs_prep_year()`. The `verbose` option returns a tibble that illustrates how file names have been modified. Once modified, they can be loaded with `cs_load_year()`:

```{r prep-load-2017}
# prep files
cs_prep_year(path = here("data", "crime", "2017"), verbose = TRUE)

# load data
data2017_raw <- cs_load_year(here("data", "crime", "2017"))
```

The `data2017_raw` is a list of tibbles, with one tibble per month for each month in a given folder. This allows for a streamlined workflow for validating and standardizing the data.

Now you try and fix the files in the `data/crime/2018` folder and then load them:

```{r prep-load-2018}
# prep files
cs_prep_year(path = here("data", "crime", "2018"), verbose = TRUE)

# load data
data2018_raw <- cs_load_year(here("data", "crime", "2018"))
```

## Validate, Standardize, and Collapse Crime Data
The data from SLMPD have un-even numbers of columns, column names, and column types depending on the year 

### 2018
We validate the data to make sure it can be collapsed using `cs_validate()`:

```{r validate-data18}
cs_validate(data2018_raw, year = "2018")
```

Since the validation result is a value of `TRUE`, we can proceed to collapsing the year-list object into a single tibble with `cs_collapse()` and then stripping out crimes reported in 2018 for earlier years using `cs_combine()`. We also strip out unfounded crimes that remain using `cs_filter_count()`:

```{r collapse-data18}
# collapse into single object
data2018_raw <- cs_collapse(data2018_raw)

# combine and filter
cs_combine(type = "year", date = 2018, data2018_raw) %>%
  cs_filter_count(var = Count) -> data2018
```

The `data2018` object now contains only crimes reported in 2018.

### 2017
We'll repeat the validation process with the 2017 data:

```{r validate-data17}
cs_validate(data2017_raw, year = "2017")
```

Since we fail the validation, we can use the `verbose = TRUE` option to get a summary of where validation issues are occuring. 

```{r validate-data17-verbose}
cs_validate(data2017_raw, year = "2017", verbose = TRUE)
```

The data for May 2017 do not pass the validation checks. We can extract this month and confirm that there are too many columns in the May 2017 release. Once we have that confirmed, we can standardize that month and re-run our validation.

```{r fix-may17}
# extract data and unit test column numbers
expect_equal(ncol(cs_extract_month(data2017_raw, month = "May")), 26)

# standardize months
data2017_raw <- cs_standardize(data2017_raw, month = "May", config = 26)

# validate data
cs_validate(data2017_raw, year = "2017")
```

We now get a `TRUE` value for `cs_validate()` and can move on to collapsing the 2017 and 2018 raw data objects to create a new object, `data2017`, that contains all known 2017 crimes including those that were reported or upgraded in 2018.

```{r collapse-data17}
# collapse into single object
data2017_raw <- cs_collapse(data2017_raw)

# combine and filter
cs_combine(type = "year", date = 2017, data2018_raw, data2017_raw) %>%
  cs_filter_count(var = Count) -> data2017
```

### Create Single Crime Table
Next, we'll create a single table before we remove individual years. We also subset columns to reduce the footprint of the table.

```{r collapse-all}
bind_rows(data2017, data2018) %>%
  select(cs_year, DateOccur, Crime, Description, ILEADSAddress, ILEADSStreet) -> crimes
```

With our `crimes` object created, we can also clean-up our global enviornment:

```{r clean-up}
rm(data2017_raw, data2017, data2018_raw, data2018)
```

### Create Single Address Field
Additionally, we want to combine the `ILEADSAddress` and `ILEADSStreet` variables into a single address string:

```{r}
crimes %>%
  combine_address() %>%
  select(-ILEADSStreet, -ILEADSAddress) %>%
  select(cs_year, address, everything()) -> crimes
```

*This functionality will be built into the next release of `compstatr`.*

## Geocode Data
Once we have our data subset and prepared, we can begin the geocoding process. The first step is to normalize the street address data. 

### Normalize Addresses
First, we'll check out the types of addresses that the `postmastr` package finds on its initial pass with `pm_identify()`:

```{r identify-crime-addresses}
# identify
crimes_pm <- pm_identify(crimes, var = address)

# evaluate
pm_evaluate(crimes_pm)
```

For simplicity's sake tonight, we're going to drop the `unknown`, `partial`, and `full` address types from the data set:

```{r}
crimes_pm <- filter(crimes_pm, pm.type == "short" | pm.type == "intersection")
```

Now, you do the same for the Citizens' Service Bureau Data. Identify the address types (using the `probaddress` variable), evaluate them, and the subset out any non `"short"` or `"intersection"` addresses:

```{r identify-csb-addresses}
# identify
csb_pm <- pm_identify(csb, var = probaddress)

# evaluate
pm_evaluate(csb_pm)

# subset
csb_pm <- filter(csb_pm, pm.type == "short" | pm.type == "intersection")
```

We need to remove two problem addresses from the CSB data as well:

```{r}
csb_pm <- filter(csb_pm, pm.uid %in% c(25337, 34985, 51193, 52708, 53787, 56486, 58849, 90282, 90353, 102210, 103177) == FALSE)
```


With our addresses identified and subset, we can now normalize them using `pm_parse()`:

```{r}
crimes_pm <- pm_parse(crimes_pm, input = "short", address = address, 
                            output = "short", new_address = addressClean, 
                            houseSuf_dict = stl_std_houseSuffix,
                            dir_dict = stl_std_directions,
                            street_dict = stl_std_streets,
                            suffix_dict = stl_std_suffix)
```

Now you try - use the same `pm_parse()` syntax to normalize the `csb_pm` data using the `probaddress` variable:

```{r}
csb_pm <- pm_parse(csb_pm, input = "short", address = probaddress, 
                            output = "short", new_address = addressClean,
                            houseSuf_dict = stl_std_houseSuffix,
                            dir_dict = stl_std_directions,
                            street_dict = stl_std_streets,
                            suffix_dict = stl_std_suffix)
```

```{r}
csb_pm %>%
  pm_prep(var = "probaddress", type = "street") %>%
  pm_house_parse() %>%
  pm_houseRange_detect() -> x
```

```{r}
x %>%
  filter(pm.hasHouseRange == TRUE) %>%
  dplyr::mutate(pm.houseVal = ifelse(pm.hasHouseRange == TRUE, pm.house, NA)) %>%
  dplyr::mutate(pm.houseVal = stringr::str_replace(pm.houseVal, pattern = "-", replacement = " ")) %>%
  dplyr::mutate(pm.houseLow = stringr::word(pm.houseVal, 1)) %>%
  dplyr::mutate(pm.houseHigh = stringr::word(pm.houseVal, 2)) %>%
  dplyr::mutate(pm.houseShort = ifelse(stringr::str_length(pm.houseLow) > stringr::str_length(pm.houseHigh), TRUE, FALSE)) %>%
  dplyr::mutate(pm.houseHigh = ifelse(pm.houseShort == TRUE,
                                      stringr::str_c(stringr::str_sub(pm.houseLow,
                                                                      start = 1,
                                                                      end = stringr::str_length(pm.houseLow)-
                                                                        stringr::str_length(pm.houseHigh)),
                                                     pm.houseHigh),
                                      pm.houseHigh)) %>%
  dplyr::mutate(pm.house2 = ifelse(pm.houseShort == TRUE, stringr::str_c(pm.houseLow, "-", pm.houseHigh), pm.house)) %>%
  dplyr::mutate(pm.house = ifelse(is.na(pm.house2) == FALSE, pm.house2, pm.house)) %>%
  dplyr::select(-pm.house2, -pm.houseShort, -pm.houseVal) -> y
```

```{r}
filter(y, suppressWarnings(as.numeric(pm.houseLow)) > suppressWarnings(as.numeric(pm.houseHigh))) -> z
```

```{r}
# address reversed ranges
    z %>%
      dplyr::mutate(pm.houseLow2 = ifelse(as.numeric(pm.houseLow) > as.numeric(pm.houseHigh), pm.houseHigh, NA)) %>%
      dplyr::mutate(pm.houseHigh2 = ifelse(as.numeric(pm.houseLow) > as.numeric(pm.houseHigh), pm.houseLow, NA)) %>%
      dplyr::mutate(pm.houseLow = ifelse(is.na(pm.houseLow2) == FALSE, pm.houseLow2, pm.houseLow)) %>%
      dplyr::mutate(pm.houseHigh = ifelse(is.na(pm.houseHigh2) == FALSE, pm.houseHigh2, pm.houseHigh)) %>%
      dplyr::select(-pm.houseLow2, -pm.houseHigh2)
```


