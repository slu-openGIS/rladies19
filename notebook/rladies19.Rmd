---
title: "RLadies 2019"
author: "Christopher Prener, Ph.D."
date: '(`r format(Sys.time(), "%B %d, %Y")`)'
output: 
  github_document: default
  html_notebook: default 
---

## Introduction
This notebook provides an overview of some of the features and tools that are at the core of the `tidystl` framework. More about `tidystl` can be found on the [SLU openGIS website](https://slu-opengis.github.io/news/tidystl_annoucement/).

## Dependencies
This notebook requires a variety of `tidystl`, `tidyverse`, and other dependencies. These reflect the ways in which I manage my own research and teach students to use `R`. There are other frameworks both for data wrangling (i.e. base `R`, `dt` (datatable)) and spatial data management (i.e. `sp`). The code chunks in the [README.md file](https://github.com/slu-openGIS/rladies19/blob/master/README.md) include installation code for all dependencies.

```{r load-packages}
# tidystl packages
library(compstatr)     # stlmpd data wrangling
library(gateway)       # st louis spatial data tools
library(postmastr)     # street address normalization
library(stlcsb)        # st louis citizens service bureau data wrangling

# tidyverse packages
library(dplyr)         # data wrangling
library(ggplot2)       # plotting
library(readr)         # read tabular data
library(stringr)       # work with strings

# spatial packages
library(mapview)       # preview spatial data
library(sf)            # spatial data tools
library(viridis)        # color palettes

# other packages
library(here)          # file path management
library(testthat)      # unit testing
```

There is also a function that is not yet part of the `compstatr` package that we'll use today:

```{r load-combine-address}
source(here("source", "combine_address.R"))
```

## Load Data
This notebook requires a number of data sets, which we'll load in groups based on their purpose.

### Mapping Data
We'll load some data that is specific for mapping:

```{r}
city <- st_read(here("data", "STL_BOUNDARY_City"), stringsAsFactors = FALSE, crs = 4269)
highway <- st_read(here("data", "STL_TRANS_PrimaryRoads"), stringsAsFactors = FALSE, crs = 4269)
```

### Geocoders
Two local geocoders are saved as `.rda` files in the `data/` to provide the most efficient storage means possible:

```{r load-geocoders}
# load local full geocoder
load(here("data", "geocoder.rda"))

# load local short geocoder
load(here("data", "geocoder_s.rda"))
```

Both geocoders could also be built using the `gateway` package:

```r
# build local full geocoder
geocoder <- gw_build_geocoder(style = "full", class = "tibble", return = "coords")

# build local short goecoder
geocoder_s <- gw_build_geocoder(style = "short", class = "tibble", return = "coords")
```

These are not built here to save time during the tutorial.

### Citizens' Service Bureau Data
A data set of non-emergency service requests (things like concerns about potholes, traffic lights that are out, and vacant buildings) for the years 2017 and 2018 is included in this repository:

```{r load-csb}
csb <- read_csv(here("data", "csb.csv"))
```

These data can also be downloaded directly via the `stlcsb` package's `csb_get_data()` function:

```r
csb <- csb_get_data(years = 2017:2018)
```

The `years` argument is optional.

### Crime Data
The crime data we'll be looking at today, also for 2017 and 2018, is included in its raw form in `data/crime/`. These data have been downloaded from the St. Louis Metropolitan Police Department's [crime statistics website](https://www.slmpd.org/Crimereports.shtml). Because of the structure of the site, we don't have a function at this time the reads the files into `R` direction from the web. They must be downloaded manually and then can be read in using the workflow below.

The `compstatr` package is "opinionated" and requires a particular organizational approach - monthly crime files should be stored as a package in dedicated yearly folders. Notice that the files have the `.html` file extension mistakenly appended:

```{r list-2017}
list.files(path = here("data", "crime", "2017"))
```

We can fix this with `cs_prep_year()`. The `verbose` option returns a tibble that illustrates how file names have been modified. Once modified, they can be loaded with `cs_load_year()`:

```{r prep-load-2017}
# prep files


# load data

```

The `data2017_raw` is a list of tibbles, with one tibble per month for each month in a given folder. This allows for a streamlined workflow for validating and standardizing the data.

Now you try and fix the files in the `data/crime/2018` folder and then load them:

```{r prep-load-2018}
# prep files


# load data

```

## Validate, Standardize, and Collapse Crime Data
The data from SLMPD have un-even numbers of columns, column names, and column types depending on the year 

### 2018
We validate the data to make sure it can be collapsed using `cs_validate()`:

```{r validate-data18}

```

Since the validation result is a value of `TRUE`, we can proceed to collapsing the year-list object into a single tibble with `cs_collapse()` and then stripping out crimes reported in 2018 for earlier years using `cs_combine()`. We also strip out unfounded crimes that remain using `cs_filter_count()`:

```{r collapse-data18}
# collapse into single object


# combine and filter

```

The `data2018` object now contains only crimes reported in 2018.

### 2017
We'll repeat the validation process with the 2017 data:

```{r validate-data17}

```

Since we fail the validation, we can use the `verbose = TRUE` option to get a summary of where validation issues are occurring. 

```{r validate-data17-verbose}

```

The data for May 2017 do not pass the validation checks. We can extract this month and confirm that there are too many columns in the May 2017 release. Once we have that confirmed, we can standardize that month and re-run our validation.

```{r fix-may17}
# extract data and unit test column numbers


# standardize months


# validate data

```

We now get a `TRUE` value for `cs_validate()` and can move on to collapsing the 2017 and 2018 raw data objects to create a new object, `data2017`, that contains all known 2017 crimes including those that were reported or upgraded in 2018.

```{r collapse-data17}
# collapse into single object


# combine and filter

```

### Create Single Crime Table
Next, we'll create a single table before we remove individual years. We also subset columns to reduce the footprint of the table.

```{r collapse-all}

```

With our `crimes` object created, we can also clean-up our global environment:

```{r clean-up}

```

## Wrangling CSB Data
First, we'll subset the number of columns to reduce the amount of data we need to wade through and make an edit to the address field:

```{r tidy-csb}
csb %>% 
  select(datetimeinit, probaddress, problemcode, status, datecancelled) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = " - ", replacement = "-")) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = "- ", replacement = "-")) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = " -", replacement = "-")) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = "MO-", replacement = "MO ")) %>%
  mutate(probaddress = str_replace(string = probaddress, pattern = "I-", replacement = "I ")) -> csb
```

We also want to remove csb calls that were cancelled. We can use the `csb_canceled()` function to do this:

```{r remove-canceled-csb}

```

Finally, we'll pull out just the calls related to degradation to explore them further. The `stlcsb` package comes with objects that are fed into the `category` argument to make subletting easier. For example, here are the call descriptions associated with degradation:

```{r degrade-descrips}
cat_degrade
```

To subset only those calls, we'll use `csb_filter()`:

```{r create-degrade}

```

## Wrangling Crime Data
Additionally, we want to combine the `ILEADSAddress` and `ILEADSStreet` variables into a single address string. We'll use a function that is included in our project's `source/` subdirectory, `combine_address()`, to do this. *This functionality will be built into the next release of `compstatr`.*

```{r}

```

With our address data combined into a single column, we'll subset the violent crime data (homicides, rapes, aggravated assaults, and robberies) for further exploration:

```{r}

```

## Geocode Data
Once we have our data subset and prepared, we can begin the geocoding process. The first step is to normalize the street address data. 

### Normalize Addresses
First, we'll check out the types of addresses that the `postmastr` package finds on its initial pass with `pm_identify()`:

```{r identify-violent-addresses}
# identify


# evaluate

```

For simplicity's sake tonight, we're going to drop the `unknown`, `partial`, and `full` address types from the data set:

```{r subset-violent-addresses}

```

Now, we'll do the same for the Citizens' Service Bureau Data. We want to identify the address types (using the `probaddress` variable), evaluate them, and the subset out any non `"short"` or `"intersection"` addresses:

```{r identify-degrade-addresses}
# identify


# evaluate


# subset

```

With our addresses identified and subset, we can now normalize them using the `postmastr` package's `pm_parse()` function. Normalizing parsed address is the process of taking un-standardized addresses and returning their standardized versions:

```{r}

```

We can use the same `pm_parse()` syntax to normalize the `csb_degrade` data using the `probaddress` variable:

```{r}

```

### Geocode
With our data normalized, we can move on geocoding them using our composite geocoder. A composite geocoder tries to match addresses using one source, and if that does not work tries a second source (and so on for as many possible sources are included in the geocoder).

There are two variants of the composite geocoder in the `gateway` package. The first is a "local" geocoder than returns only results that are matched in the two local geocoder objects, `geocoder` and `geocoder_s` in this case:

```{r}

```

Alternatively, we can use a geocoder that calls the City of St. Louis's address candidate API. The API returns results like this:

```{r}

```

The composite geocoder uses a minimum threshold of 90% by default, but this can be changed. The following syntax would geocode the violent crime data:

```r
crime_violent <- gw_geocode_composite(crime_violent, var = addressClean,
                                        local_geocoder = geocoder, short_geocoder = geocoder_s,
                                        local = FALSE, osm = FALSE)
```

And this syntax would geocode the CSB data:

```r
csb_degrade <- gw_geocode_composite(csb_degrade, var = addressClean,
                                        local_geocoder = geocoder, short_geocoder = geocoder_s,
                                        local = FALSE, osm = FALSE)
```

The pre-geocoded versions can be loaded here:

```{r load-pre-geocoded-data}
load(here("data", "crime_violent.rda"))
load(here("data", "csb_degrade.rda"))
```

## Projecting
We need to convert our geocoded data to an `sf` object. This is a two part process. We need to first remove data missing coordinates, and then project it using `sf`'s `st_as_sf()` function: 

```{r project-crime}

```

We'll do the same with the CSB data:

```{r project-csb}

```

## Aggregating
One thing we often want to know is how many crimes or incidents occur in a particular neighborhood. We can aggregate our individual incidents up to the neighborhood they occurred in to get counts using the `gateway` package's `gw_aggregate()` function:

```{r aggregate-violent}

```

We can do the same for CSB incidents:

```{r aggregate-degrade}

# create copy

# remove geometry
```

## Plotting
One way to explore the relationship between our two data sets is a scatter plot, which we can make with `ggplot2`:

```{r compare-crime-degrade}

```


## Mapping
With our data projected, we can move on to mapping. I like the `mapview` package for previewing data. For example, we can look at our point data:

```{r preview-point}

```

Or we can preview our polygon data:

```{r preview-nhood}

```

We can also map these using `ggplot2`. For example:

```{r map-violent}

```

